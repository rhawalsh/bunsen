---
# If provisioning is not using a storage server we generally expect there to be
# an auxiliary disk (/dev/sdb or /dev/vdb depending on virtualization type, if
# any) that provides the necessary storage.  If such exists we use it.  If it
# does not we consider the following in order:
#   - if /home is a mount point create a loop device using a file
#   - if there is a system root logical volume group use it to create a logical
#     volume
#
# If using a storage server we expect it to provide the necessary storage.
#
# The expectation is that the above provides the necessary storage.  If it
# does not provisioning will fail.

- block:
  - name: Create /u1 mount point
    file:
      path: /u1
      state: directory
      mode: 0755
    when: "'/u1' not in mount_points"
    become: yes

  # Not using a storage server.
  - block:
    - block:
      # Use the auxiliary disk, if it exists.
      - name: Setting scratch volume group and test disk names
        set_fact:
          scratch_vg: scratch
          test_disk: "/dev/{{aux_disk}}"

      vars:
        aux_disk: "{%- if ansible_virtualization_type == 'kvm' -%}
                     vdb
                   {%- else -%}
                     sdb
                   {%- endif -%}"
      when: aux_disk in ansible_devices.keys()

    - block:
        # scratch_vg not defined and system has a /home mount.
        # Create a big file on /home and use it via a loop device as a
        # physical volume.
        - include_tasks: storage-loop.yml
          vars:
            mount: home
            path: big_file
            min_size: "{{ required }}"
            max_size: "{{ (vdo_scratch_max_gb * 1024 * 1024) | int }}"
            # test_disk and scratch_vg are set by storage-loop.yml.
            disk_var: test_disk
            volume_group: scratch_vg
      when: (scratch_vg is not defined) and ('/home' in mount_points)

    - block:
        # scratch_vg not defined and system with a system volume group.
        # Assume any free space lives within the system volume group.
        - name: Using system volume group name for scratch storage
          set_fact:
            scratch_vg: "{{ ansible_lvm.lvs['root'].vg }}"
            test_disk: ""
      when: (scratch_vg is not defined)
              and (ansible_lvm is defined)
              and ('root' in ansible_lvm.lvs.keys())

    - block:
        # scratch_vg not defined.
        # Create a big file on / and use it via a loop device as a
        # physical volume.
        - include_tasks: storage-loop.yml
          vars:
            mount: /
            path: big_file
            min_size: "{{ required }}"
            max_size: "{{ (vdo_scratch_max_gb * 1024 * 1024) | int }}"
            # test_disk and scratch_vg are set by storage-loop.yml.
            disk_var: test_disk
            volume_group: scratch_vg
      when: (scratch_vg is not defined)
    when: (not use_storage_server) or (inventory_hostname not in storage_clients)

  # System with an external storage server specified: Use it.
  - block:
    - include_tasks: storage-targetd.yml
      vars:
        # test_disk is set by storage-targetd.yml.
        disk_fact: test_disk
        ipaddr: "{{ lookup('dig', storage_server) }}"
        size: "{{ (required|int) * 1024 }}"
        targetd_user: "{{ storage_server_account }}"
        targetd_password: "{{ storage_server_password }}"
        targetd_args: "--host {{ storage_server }} \
                       --user {{ targetd_user }} \
                       --password {{ targetd_password }}"

    - name: Setting scratch volume group name
      set_fact:
        scratch_vg: "{{ ansible_default_ipv4.macaddress.split(':') | join }}"

    when: use_storage_server and (inventory_hostname in storage_clients)

  # If we ran across something unexpected, we won't get the results we want.
  - name: Verifying storage variables have been set
    assert:
      that:
        - scratch_vg is defined
        - test_disk is defined

  - name: Add physical volume to volume group
    lvg:
      vg: "{{ scratch_vg }}"
      pvs: "{{ test_disk }}"
    when: test_disk != ""
    become: yes

  - name: Create logical volume for /u1
    lvol:
      vg: "{{ scratch_vg }}"
      lv: "{{ u1_lv }}"
      size: "{{ farm_u1_size_gb }}G"
    become: yes

  - name: Create file system for /u1
    filesystem:
      fstype: xfs
      dev: "/dev/{{ u1_lv_full }}"
    become: yes

  - name: Mount u1
    mount:
      name: /u1
      src: "/dev/{{ u1_lv_full }}"
      fstype: xfs
      state: mounted
      opts: "{%- if use_storage_server
                    and (inventory_hostname in storage_clients) -%}
               _netdev
             {%- else -%}
               defaults
             {%- endif -%}"
    become: yes

  - name: Adjust mounted /u1 permissions
    file:
      path: /u1
      state: directory
      mode: 01777
    become: yes

  - block:

      - name: Query for existence of vdo_scratch device
        stat:
          path: "{{ full_path }}"
        register: stat_result

      - name: Verify vdo_scratch device presence
        assert:
          that:
            - stat_result.stat.isblk is defined
            - stat_result.stat.isblk
          fail_msg: "{{ full_path }} must be an existing block device"

    vars:
      full_path: "/dev/{{ test_storage_device }}"
    when: test_storage_device is defined

  # We've changed the name to have the numeric prefix. Remove the old file, in
  # case anyone tries to update an installation made with the old
  # configuration.
  - name: Remove old vdo_scratch.rules file
    file:
      path: /etc/udev/rules.d/vdo_scratch.rules
      state: absent
    become: yes

  - name: Set up vdo_scratch udev rule
    # For a raw device, use KERNEL=="vdb" or whatever, but for device-mapper
    # devices (/dev/dm-##) we don't want to make assumptions about numbering.
    template:
      src: vdo_scratch.rules
      dest: /etc/udev/rules.d/99-vdo_scratch.rules
      mode: 0644
      owner: root
    register: udev_result
    become: yes

  - block:
      - name: Reload udev rules
        command: udevadm control --reload
        become: yes

      - name: Trigger udev processing
        command: udevadm trigger
        become: yes

      # Should we wait (udevadm settle), and verify that the link has been
      # created?

    when: udev_result is changed

  - setup:
      filter: ansible_mounts
    become: yes

  - setup:
      filter: ansible_lvm
    become: yes

  - name: Create scratch logical volume for testing
    lvol:
      vg: "{{ scratch_vg }}"
      lv: "{{ scratch_lv }}"
      size: "{{ (free_gb | float < vdo_scratch_min_gb)
                | ternary(vdo_scratch_min_gb | string + 'G',
                          (free_gb | float > vdo_scratch_max_gb)
                          | ternary(vdo_scratch_max_gb | string + 'G',
                                    '100%FREE')) }}"
    when: ((scratch_lv_full not in existing_lvs)
           and (test_storage_device is not defined))
    become: yes
    vars:
      free_gb: "{{ ansible_lvm.vgs[scratch_vg].free_g }}"

  vars:
    vdo_scratch_min_gb: 100
    # Cap test storage at around 150 GB based on test configs.
    vdo_scratch_max_gb: 150
    existing_lvs: "
      {%- set tmp = {} -%}
      {%- set lvs = (ansible_lvm is defined) | ternary(ansible_lvm.lvs, {}) -%}
      {%- for lv in lvs -%}
        {%- set x = tmp.update({ lvs[lv].vg+'/'+lv : 1 }) -%}
      {%- endfor -%}
      {{ tmp.keys() | list }}"
    scratch_lv: "vdo_scratch"
    scratch_lv_full: "{{ scratch_vg }}/{{ scratch_lv }}"
    # checkServer.pl expects the LV for /u1 to be named "scratch", if it's an
    # LV at all and not a plain partition.
    u1_lv: "scratch"
    u1_lv_full: "{{ scratch_vg }}/{{ u1_lv }}"
    # kB required for u1 + vdo_scratch + slop
    required: "{{ ((farm_u1_size_gb | int) + vdo_scratch_min_gb + 1) * 1024 * 1024 }}"
    # mount point directory names
    mount_points: "{{ ansible_mounts | map(attribute='mount') | list }}"

